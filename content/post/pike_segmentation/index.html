---
title: U-Net semantic segmentation with PyTorch
author: Kenneth
date: '2021-01-19'
categories:
  - Machine learning
  - Python
tags:
  - PyTorch
  - deep learning
  - image analysis
slug: pike_segmentation
lastmod: '2021-01-19T11:55:27+01:00'
featured: no
image: 
  caption: ''
  placement: ''
  focal_point: ''
  preview_only: yes
---



<p>Deep learning is here to stay and has revolutionized the way data is analyzed. Furthermore, it is straightforward to get started. Recently, <a href="https://www.datainwater.com/post/fish_cnn/">I played around with the fastai library</a> to classify fish species but wanted to go further behind the scenes and dig deeper into PyTorch. As part of another project, I have used a U-Net to perform semantic segmentation of ‘pike’ in images. Training has been done on Google Colab and a local GPU powered workstation excellent for smaller experiments.</p>
<p>Here, I walk through the steps to do the analysis and a notebook is also available on <a href="https://github.com/KennethTM/datainwater/tree/master/content/post/pike_segmentation/python/">Github</a>.</p>
<div id="the-data" class="section level3">
<h3>The data</h3>
<p>Data consists of images of northern pike (<em>Esox lucius</em>). 60 images have been annotated using <a href="https://www.robots.ox.ac.uk/~vgg/software/via/">VIA</a> where regions of ‘pike’ have been delineated using poly-lines. These annotations are used to produce the target masks needed for training. The first part thus consists of loading image and annotation data and convert polygon annotations to images. Furthermore, the original high-resolution images are resized to a common maximum side length (1024 pixels), while maintaining the aspect ratio, to reduce the workload for image IO during training.</p>
<p>Load libraries and data and define paths:</p>
<pre class="python"><code>#import libraries and define paths
import os
import sys
import json
import numpy as np
from skimage.io import imread, imsave
from skimage.transform import resize
from skimage.draw import draw
import matplotlib.pyplot as plt
import albumentations as A
import cv2
import random
import torch
from albumentations.pytorch import ToTensorV2
from unet_network import UNet
import time
import copy

data_dir = &#39;../data/&#39;
image_dir = os.path.join(data_dir, &quot;images_pike&quot;)
image_small_dir = os.path.join(data_dir, &quot;images_pike_small&quot;)
mask_dir = os.path.join(data_dir, &quot;images_pike_masks&quot;)
mask_small_dir = os.path.join(data_dir, &quot;images_pike_masks_small&quot;)
via_proj = os.path.join(data_dir, &quot;via_project.json&quot;)

img_longest_size = 1024

#get VIA annotations from json file
annotations = json.load(open(via_proj)) 
annotations = list(annotations[&#39;_via_img_metadata&#39;].values())
annotations = [a for a in annotations if a[&#39;regions&#39;]]

#function for reducing high resolution images to have a maximum height or 
#width no longer than max_length
def img_max_length(img, max_length, mode):
    
    heigth, width = img.shape[:2]
    aspect = width/float(heigth)
    
    if (mode == &quot;img&quot;):
        kwargs = {&quot;order&quot;: 1, &quot;anti_aliasing&quot;: True, &quot;preserve_range&quot;: False}
    elif (mode == &quot;mask&quot;):
        kwargs = {&quot;order&quot;: 0, &quot;anti_aliasing&quot;: False, &quot;preserve_range&quot;: True}
    else:
        return(1)
    
    if (heigth &gt; max_length or width &gt; max_length):
        if(aspect&lt;1):
        #landscape orientation - wide image
            res = int(aspect * max_length)
            img_small = resize(img, (max_length, res), **kwargs)
        elif(aspect&gt;1):
        #portrait orientation - tall image
            res = int(max_length/aspect)
            img_small = resize(img, (res, max_length), **kwargs)
        elif(aspect == 1):
            img_small = resize(img, (max_length, max_length), **kwargs)
        else:
            return(1)
        if (mode == &quot;mask&quot;):
            img_small = img_small.astype(&quot;uint8&quot;)

        return(img_small)
    else:
        return(img)</code></pre>
<p>Create image masks from polygons:</p>
<pre class="python"><code>#create masks and write to file in both original and reduced size
for a in annotations:
    polygons = [r[&#39;shape_attributes&#39;] for r in a[&#39;regions&#39;]] 
    image_path = os.path.join(image_dir, a[&#39;filename&#39;])
    image = imread(image_path)
    heigth, width = image.shape[:2]
    mask = np.zeros([heigth, width], dtype=np.uint8)
    
    rr, cc = draw.polygon(polygons[0][&#39;all_points_y&#39;], polygons[0][&#39;all_points_x&#39;])
    mask[rr, cc] = 1
    
    mask_name = a[&#39;filename&#39;].replace(&quot;.jpg&quot;, &quot;.png&quot;)
    mask_path = os.path.join(mask_dir, mask_name)
    imsave(mask_path, mask)
    
    mask_small_path = os.path.join(mask_small_dir, mask_name)
    mask_small = img_max_length(mask, img_longest_size, mode = &quot;mask&quot;)
    imsave(mask_small_path, mask_small)
  
#create reduced sized versions of high res images
images_raw = list(sorted(os.listdir(os.path.join(data_dir, &quot;images_pike&quot;))))

for i in images_raw:
    img_path = os.path.join(image_dir, i)
    img = imread(img_path)
    
    img_name = i.replace(&quot;.jpg&quot;, &quot;.png&quot;)
    img_small_path = os.path.join(image_small_dir, img_name)
    img_small = img_max_length(img, img_longest_size, mode = &quot;img&quot;)
    imsave(img_small_path, img_small)</code></pre>
</div>
<div id="setup-model-and-dataset" class="section level3">
<h3>Setup model and dataset</h3>
<p>The model is a <a href="https://github.com/mateuszbuda/brain-segmentation-pytorch">U-Net implementation</a> where the input is a 3 channel image and output is a segmentation mask with pixel values from 0-1.</p>
<p>To load the data, we extend the PyTorch Dataset class:</p>
<pre class="python"><code>#define dataset for pytorch
class PikeDataset(torch.utils.data.Dataset):
    def __init__(self, images_directory, masks_directory, mask_filenames, transform=None):
        self.images_directory = images_directory
        self.masks_directory = masks_directory
        self.mask_filenames = mask_filenames
        self.transform = transform

    def __len__(self):
        return len(self.mask_filenames)

    def __getitem__(self, idx):
        mask_filename = self.mask_filenames[idx]
        image = cv2.imread(os.path.join(self.images_directory, mask_filename))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        mask = cv2.imread(os.path.join(self.masks_directory, mask_filename), 
                          cv2.IMREAD_UNCHANGED)
        mask = np.expand_dims(mask, 2)
        mask = mask.astype(np.float32)
        
        if self.transform is not None:
            transformed = self.transform(image=image, mask=mask)
            image = transformed[&quot;image&quot;]
            mask = transformed[&quot;mask&quot;]
            
        return image, mask</code></pre>
<p>To improve training and generalization, image augmentation is applied. This procedure generates images with random distortions or modifications. The <a href="https://albumentations.ai/">Albumentations</a> library makes this process easy to apply simultaneously for image and mask. Here, transformations are defined for the training set and just resizing and normalization for the validation set:</p>
<pre class="python"><code>#define augmentations
train_transform = A.Compose([
    A.Resize(400, 400, always_apply=True),
    A.RandomCrop(height=200, width = 200, p=0.2),
    A.PadIfNeeded(min_height=400, min_width=400, border_mode=cv2.BORDER_CONSTANT, 
                  always_apply=True),
    A.VerticalFlip(p=0.2),              
    A.Blur(p=0.2),
    A.RandomRotate90(p=0.2),
    A.ShiftScaleRotate(p=0.2, border_mode=cv2.BORDER_CONSTANT),
    A.RandomBrightnessContrast(p=0.2),
    A.RandomSunFlare(p=0.2, src_radius=200),
    A.RandomShadow(p=0.2),
    A.RandomFog(p=0.2),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2(transpose_mask=True)
]
)

val_transform = A.Compose([
    A.Resize(400, 400, always_apply=True), 
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), 
    ToTensorV2(transpose_mask=True)
]
)</code></pre>
<p>And finally, the dataset is split into a training (50) and validation set (10):</p>
<pre class="python"><code>#split files at random into training and validation datasets
mask_filenames = list(sorted(os.listdir(mask_small_dir)))

random.seed(43)
random.shuffle(mask_filenames)

n_val = 10

val_mask_filenames = mask_filenames[:10]
train_mask_filenames = mask_filenames[10:]

train_dataset = PikeDataset(image_small_dir, mask_small_dir, train_mask_filenames, 
                            transform=train_transform)

val_dataset = PikeDataset(image_small_dir, mask_small_dir, val_mask_filenames, 
                          transform=val_transform)</code></pre>
</div>
<div id="training-the-model" class="section level3">
<h3>Training the model</h3>
<p>Before training, dataloaders are created from the datasets and a few things are defined: a loss function (diceloss), hyperparameters (epochs, learning rate), model instantiated and moved to device (e.g. GPU):</p>
<pre class="python"><code>#dice loss function
class DiceLoss(torch.nn.Module):
    def __init__(self):
        super(DiceLoss, self).__init__()
        self.smooth = 1.0

    def forward(self, y_pred, y_true):
        assert y_pred.size() == y_true.size()
        y_pred = y_pred[:, 0].contiguous().view(-1)
        y_true = y_true[:, 0].contiguous().view(-1)
        intersection = (y_pred * y_true).sum()
        dsc = (2.*intersection+ elf.smooth)/(y_pred.sum()+y_true.sum()+self.smooth)
        return 1. - dsc
        
#function to return data loaders
def data_loaders(dataset_train, dataset_valid, bs = 5, workers = 5):

    def worker_init(worker_id):
        np.random.seed(42 + worker_id)

    loader_train = torch.utils.data.DataLoader(
        dataset_train,
        batch_size=bs,
        shuffle=True,
        drop_last=True,
        num_workers=workers,
        worker_init_fn=worker_init,
    )
    
    loader_valid = torch.utils.data.DataLoader(
        dataset_valid,
        batch_size=bs,
        drop_last=False,
        shuffle=False,
        num_workers=workers,
        worker_init_fn=worker_init,
    )

    return {&quot;train&quot;: loader_train, &quot;valid&quot;: loader_valid}
    
#define datasets and load network from file
loaders = data_loaders(train_dataset, val_dataset)

device = torch.device(&quot;cpu&quot; if not torch.cuda.is_available() else &quot;cuda&quot;)

unet = UNet(in_channels=3, out_channels=1, init_features=8)
unet.to(device)

dsc_loss = DiceLoss()
best_validation_dsc = 1.0

lr = 0.001
epochs = 200

optimizer = torch.optim.Adam(unet.parameters(), lr=lr)

best_val_true = []
best_val_pred = []
epoch_loss = {&quot;train&quot;: [], &quot;valid&quot;: []}</code></pre>
<p>Then, the the model can be trained in a couple of minutes (depending on hardware). Best models are saved and training can be monitored from the validation loss:</p>
<pre class="python"><code>#training loop 
for epoch in range(epochs):
    print(&#39;-&#39; * 100)
    since = time.time()
    best_model_wts = copy.deepcopy(unet.state_dict())
    
    for phase in [&quot;train&quot;, &quot;valid&quot;]:
        
        if phase == &quot;train&quot;:
            unet.train()
        else:
            unet.eval()

        epoch_samples = 0
        running_loss = 0
        
        for i, data in enumerate(loaders[phase]):
            x, y_true = data
            x, y_true = x.to(device), y_true.to(device)
            
            epoch_samples += x.size(0)
            
            optimizer.zero_grad()
            
            with torch.set_grad_enabled(phase == &quot;train&quot;):
                y_pred = unet(x)
                loss = dsc_loss(y_pred, y_true)
                running_loss += loss.item()
                    
                if phase == &quot;train&quot;:
                    loss.backward()
                    optimizer.step()

        epoch_phase_loss = running_loss/epoch_samples
        epoch_loss[phase].append(epoch_phase_loss)
    
        if phase == &quot;valid&quot; and epoch_phase_loss &lt; best_validation_dsc:
            best_validation_dsc = epoch_phase_loss
            print(&quot;Saving best model&quot;)
            best_model_wts = copy.deepcopy(unet.state_dict())
            best_val_true = y_true.detach().cpu().numpy()
            best_val_pred = y_pred.detach().cpu().numpy()
        
    time_elapsed = time.time() - since
    print(&quot;Epoch {}/{}: Train loss = {:4f} --- Valid loss = {:4f} --- Time: {:.0f}m {:.0f}s&quot;.format(epoch + 1, epochs, epoch_loss[&quot;train&quot;][epoch], epoch_loss[&quot;valid&quot;][epoch], time_elapsed // 60, time_elapsed % 60))

print(&quot;Best validation loss: {:4f}&quot;.format(best_validation_dsc))
torch.save(best_model_wts, os.path.join(data_dir, &quot;unet.pt&quot;))</code></pre>
<p>Training and validation run from 200 epochs:</p>
<p><img src="/img/segmentation_post_training.png" /></p>
</div>
<div id="using-the-model" class="section level3">
<h3>Using the model</h3>
<p>After training, the model weights are saved to a file. These can be re-loaded and used with the network for producing predictions for new samples:</p>
<pre class="python"><code>#load weights to network
weights_path = data_dir + &quot;unet.pt&quot;
device = &quot;cpu&quot;

unet = UNet(in_channels=3, out_channels=1, init_features=8)
unet.to(device)
unet.load_state_dict(torch.load(weights_path, map_location=device))</code></pre>
<p>Then, a couple of utility functions are loaded to resize the image to fit the network, apply predictions and putting it all together:</p>
<pre class="python"><code>#define augmentations 
inference_transform = A.Compose([
    A.Resize(400, 400, always_apply=True),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), 
    ToTensorV2()
])

#define function for predictions
def predict(model, img, device):
    model.eval()
    with torch.no_grad():
        images = img.to(device)
        output = model(images)
        predicted_masks = (output.squeeze() &gt;= 0.5).float().cpu().numpy()
        
    return(predicted_masks)

#define function to load image and output mask
def get_mask(img_path):
    image = cv2.imread(img_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    original_height, original_width = tuple(image.shape[:2])
    
    image_trans = inference_transform(image = image)
    image_trans = image_trans[&quot;image&quot;]
    image_trans = image_trans.unsqueeze(0)
    
    image_mask = predict(unet, image_trans, device)
    image_mask = F.resize(image_mask, original_height, original_width, 
                          interpolation=cv2.INTER_NEAREST)
        
    return(image_mask)</code></pre>
<p>Finally. the predictions for a new image, which has not been used for training:</p>
<pre class="python"><code>#image example
example_path = &quot;159_R_1.png&quot;
image = cv2.imread(example_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

mask = get_mask(example_path)

masked_img = image*np.expand_dims(mask, 2).astype(&quot;uint8&quot;)

#plot the image, mask and multiplied together
fig, (ax1, ax2, ax3) = plt.subplots(3)

ax1.imshow(image)
ax2.imshow(mask)
ax3.imshow(masked_img)</code></pre>
<p><img src="/img/segmentation_post_result.png" />
<em>The result after training a U-Net to perform semantic segmentation of ‘pike’. The top is the new and unseen (by the model) image, the middle is the predicted mask region and the bottom is the two multiplied to apply the mask</em></p>
</div>
<div id="conclusions" class="section level3">
<h3>Conclusions</h3>
<ul>
<li>Image-augmentation improves training and Albumentations makes this easy</li>
<li>Images can quickly be annotated using VIA</li>
<li>U-Net does not require large datasets and performs well</li>
<li>Getting ‘under the hood’ using PyTorch directly is a learn full experiences</li>
</ul>
</div>
